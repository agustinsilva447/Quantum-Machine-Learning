{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning quantum DD compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "1. **Model training.** Two deep learning models were trained on 1,000 random 5-qubit circuits to predict the difference two equivalent random circuits' Hamming distance to the ideal output. One model was trained on IBM Q Burlington, and one model was trained on IBM Q London. These supremacy-style random circuits had many gaps due to 2-qubit gates compensating for partial connectivity after compiling to maximum optimization; we filled the gaps completely randomly with sequences of U3 gates equivalent to the identity (i.e. random dynamical decoupling sequences).\n",
    "\n",
    "2. **Compiling.** We ran the trained model on a new set of 500 random circuits. For each random circuit, we generated 1,000 randomly padded circuits. The deep learning model was then used to predict the relative noise between pairs of circuits in 50 tournaments of 20 competitors each, selecting the circuit expected to be the least noisy as the \"compiled\" circuit.\n",
    "\n",
    "3. **Testing.** The compiled circuits were run on IBM Q Burlington, Essex, London, Ourense, Vigo (which have the same architecture) and Yorktown (which has a different architecture). When the compiled circuit corresponded to the device for which the noise model was trained, we observed a 12% (95% CI [12%, 13%]). On all other 5-qubit devices, the deep learning compiler also performed better than the IBM Qiskit compiler, but only by 5.2% (95% CI [4.9%, 5.6%]). This is shown to be significantly better than random dynamical decoupling sequences and the generally used XYXY sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bootstrapped.bootstrap as bs\n",
    "import bootstrapped.stats_functions as bs_stats\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average impact of random DD sequences on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burlington -0.04472841360952699    (-0.045877976501480885, -0.043586562159673985)\n",
      "London -0.07317828321117872    (-0.07469895131131679, -0.07161936271596131)\n"
     ]
    }
   ],
   "source": [
    "def average_dd(file):\n",
    "    a = np.load(file)\n",
    "    free = np.expand_dims(a[:, 0], 1)\n",
    "    compiled = a[:, 1:]\n",
    "    return bs.bootstrap(((compiled - free)/free).flatten(), stat_func=bs_stats.mean)\n",
    "\n",
    "print('Burlington', average_dd('supremacy_all_5_unique/burlington_noise.npy'))\n",
    "print('London', average_dd('supremacy_all_5_unique/london_noise.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the change in noise as determined by Hamming distance between compiled and identity for all computers. Here we also show the difference in noise between a given device and the device for which the noise model was trained for (either IBM Q Burlington or IBM Q London). This analysis demonstrates that the noise model is device-specific with over 95% confidence for all comparisons to other devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(files):\n",
    "    data = []\n",
    "    for i in range(len(files)):\n",
    "        d = []\n",
    "        for f in sorted(files[i]):\n",
    "            d.append(np.load(f))\n",
    "        if len(d) == 0:\n",
    "            return None\n",
    "        data.append(np.concatenate(d))\n",
    "    diff = (data[0][:len(data[1])] - data[1])/data[1]\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- MODEL TRAINED ON burlington ----------\n",
      "burlington change in Hamming distance -0.11140341171900789    (-0.12424162473869524, -0.09785457534663462)\n",
      "\n",
      "Device: essex\n",
      "essex change in noise -0.05544860532039153    (-0.07066875273166959, -0.0426917043367816)\n",
      "noise difference from burlington 0.0495800995168085    (0.03855165660877469, 0.06416576698036144)\n",
      "\n",
      "Device: london\n",
      "london change in noise -0.05561113504815794    (-0.05954463235886391, -0.04937955851956487)\n",
      "noise difference from burlington 0.04336468345014906    (0.028038757397818553, 0.054271579530622165)\n",
      "\n",
      "Device: ourense\n",
      "ourense change in noise -0.06148141545139791    (-0.07027990407352763, -0.05256197981566858)\n",
      "noise difference from burlington 0.03574368774382461    (0.024658910206309573, 0.04752435282444325)\n",
      "\n",
      "Device: vigo\n",
      "vigo change in noise -0.047762428196207886    (-0.056711075319585455, -0.03707863334554757)\n",
      "noise difference from burlington 0.04772189166971301    (0.025048129892091833, 0.06569951600202092)\n",
      "\n",
      "Device: yorktown\n",
      "yorktown change in noise -0.06472277844406726    (-0.07633802647050875, -0.055579921995415504)\n",
      "noise difference from burlington 0.03736205523040374    (0.018496368331078067, 0.050766748096508996)\n",
      "\n",
      "\n",
      "---------- MODEL TRAINED ON london ----------\n",
      "london change in Hamming distance -0.13421316006786976    (-0.14144564325310202, -0.12686147878892445)\n",
      "\n",
      "Device: burlington\n",
      "burlington change in noise -0.07649346594194084    (-0.09135206906025439, -0.06525658064746655)\n",
      "noise difference from burlington 0.047282889482870696    (0.03612709390819341, 0.06439923063183309)\n",
      "\n",
      "Device: essex\n",
      "essex change in noise -0.07911763353170495    (-0.09916373048757807, -0.06387556504141643)\n",
      "noise difference from burlington 0.05386933311508781    (0.03499035594897769, 0.07171903045089374)\n",
      "\n",
      "Device: ourense\n",
      "ourense change in noise -0.05749673052306089    (-0.06525252840363474, -0.04680756284954361)\n",
      "noise difference from burlington 0.06370911185187411    (0.04613866179215241, 0.07391609776528921)\n",
      "\n",
      "Device: vigo\n",
      "vigo change in noise -0.04061741469928498    (-0.05341020882521416, -0.03198651420935285)\n",
      "noise difference from burlington 0.09011040433490836    (0.07787030939615996, 0.1000877469456338)\n",
      "\n",
      "Device: yorktown\n",
      "yorktown change in noise -0.05373735390424967    (-0.06547612845938522, -0.04670801870641156)\n",
      "noise difference from burlington 0.09283553102544004    (0.08351282007325028, 0.10225944623703517)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare(computer):\n",
    "    home = 'test_noise_5_' + computer + '/'\n",
    "    home_files = [glob.glob(home + computer + '_compiled*'), glob.glob(home + computer + '_identity*')]\n",
    "    home_diff = diff(home_files)\n",
    "\n",
    "    print('---------- MODEL TRAINED ON', computer, '----------')\n",
    "    print(computer, 'change in Hamming distance', bs.bootstrap(home_diff, stat_func=bs_stats.mean))\n",
    "    print()\n",
    "\n",
    "    comparisons = ['burlington', 'essex', 'london', 'ourense', 'vigo', 'yorktown']\n",
    "    comparisons.remove(computer)\n",
    "    all_diffs = []\n",
    "    for c in comparisons:\n",
    "        files = [glob.glob(home + c + '_compiled*'), glob.glob(home + c + '_identity*')]\n",
    "        dr_diff = diff(files)\n",
    "        all_diffs.append(dr_diff)\n",
    "        print('Device:', c)\n",
    "        if dr_diff is None:\n",
    "            print('no data')\n",
    "        else:\n",
    "            print(c + ' change in noise', bs.bootstrap(dr_diff, stat_func=bs_stats.median))\n",
    "            print('noise difference from burlington', bs.bootstrap(dr_diff - home_diff, stat_func=bs_stats.median))\n",
    "        print()\n",
    "    print()\n",
    "    \n",
    "    return home_diff, np.concatenate(all_diffs)\n",
    "    \n",
    "b_home, b_diffs = compare('burlington')\n",
    "l_home, l_diffs = compare('london')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can aggregate these individual device results into two numbers:\n",
    "* 12.3% (95% [11.5%, 13.0%]): the percent improvement in Hamming distance from the observed output to the ideal output of the deep learning compiled circuit minus the Qiskit compiled circuit _on the device for which deep learning was trained_\n",
    "* 5.2% (95% [4.9%, 5.6%]): the percent improvement in Hamming distance from the observed output to the ideal output of the deep learning compiled circuit minus the Qiskit compiled circuit _on the device for which deep learning was **not** trained_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total deep learning noise -0.12280828589343883    (-0.13044564003565168, -0.11521732448599387)\n",
      "average improvement on different device -0.05224541626483875    (-0.05577127586622978, -0.04875028977743232)\n"
     ]
    }
   ],
   "source": [
    "print('total deep learning noise', bs.bootstrap(np.concatenate((b_home, l_home)), stat_func=bs_stats.mean))\n",
    "print('average improvement on different device', bs.bootstrap(np.concatenate((b_diffs, l_diffs)), stat_func=bs_stats.mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the compiled circuits for which all gaps were multiples of 4, we can check how the deep learning compilation compares to inserting XYXY sequences, as is common in dynamical decoupling. Deep learning is found to improve noise 6.5% (95% CI [2.1%, 10.6%]) percentage points better than XYXY padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06467127855852302    (0.021068539533414254, 0.10577344875083491)\n"
     ]
    }
   ],
   "source": [
    "b_xyxy = np.load('test_noise_xyxy_subset/burlington_compiled_00000.npy') - np.load('test_noise_xyxy_subset/burlington_identity_00000.npy')\n",
    "l_xyxy = np.load('test_noise_xyxy_subset/london_compiled_00000.npy') - np.load('test_noise_xyxy_subset/london_identity_00000.npy')\n",
    "b_xyxy_candidates = np.array([16, 182, 246, 263, 264, 303, 354, 396, 419, 482])\n",
    "l_xyxy_candidates = np.array([182, 246, 264, 303, 354, 396, 419])\n",
    "print(bs.bootstrap(np.concatenate((l_xyxy - l_home[l_xyxy_candidates], b_xyxy - b_home[b_xyxy_candidates])), stat_func=bs_stats.mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
